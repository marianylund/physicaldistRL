{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Covid19Environment import GraphicCovid19Environment, Covid19Environment, States, Actions\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Reinforcement Learning (Kandidatnr: 10114)\n",
    "\n",
    "To explain it I came up with an important survival skill we all have today. The ability to go to Kiwi and back without meeting another person. Thus we are creating a survivor that can stand on one tile and see only one tile in front. Survivor can make 3 actions, look left, look right and move forward.\n",
    "\n",
    "We create a Q-table of possible states. It becomes a 5 x 5 x 3. First dimension for its standing state, 2nd for for s/he sees and 3 possible actions to take.\n",
    "\n",
    "### Q Table\n",
    "- Stand_states: null, someone, kiwi, house, border (5)\n",
    "- See_states: null, someone, kiwi, house, border (5)\n",
    "- Actions: left, right, forward (3)\n",
    "\n",
    "### Rewards:\n",
    "- Taking 1 action: -1 (faster is better)\n",
    "- Standing on Kiwi: +50 if it is first time, then 0 (it is safe to look around whilst inside)\n",
    "- Standing with person: -10 (you might get infected)\n",
    "- Getting home: +30 or +200 if surviver has been to Kiwi, since that is the goal\n",
    "- Stand on border: -1000 (do not want it to get out of the radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_others(num):\n",
    "    possibilities = []\n",
    "    for i in range(world_size[0]):\n",
    "        for j in range(world_size[1]):\n",
    "            possibilities.append([i, j])\n",
    "    possibilities.remove([0, 0])\n",
    "    possibilities.remove([world_size[0]-1, world_size[1]-1])\n",
    "    possibilities.remove([math.floor(world_size[0]/2), math.floor(world_size[1]/2)]) # remove start pos\n",
    "\n",
    "    others = []\n",
    "    for i in range(num):\n",
    "        rand = random.choice(possibilities)\n",
    "        others.append(rand)\n",
    "        possibilities.remove(rand)\n",
    "    return others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Q Table:\n",
      " [[[ -2.52  -2.51  32.85]\n",
      "  [  4.02   0.76 162.92]\n",
      "  [ -2.18   4.76 -14.19]\n",
      "  [ -2.1   -0.89  48.66]\n",
      "  [ -1.97   3.41   -inf]]\n",
      "\n",
      " [[  0.     0.     0.  ]\n",
      "  [  0.     0.     0.  ]\n",
      "  [  0.     0.     0.  ]\n",
      "  [  0.     0.     0.  ]\n",
      "  [  0.     0.     -inf]]\n",
      "\n",
      " [[-14.36 -13.82   1.74]\n",
      "  [ -6.33   0.35  44.92]\n",
      "  [-13.88  -8.74  -7.72]\n",
      "  [ -7.66 -10.41   1.37]\n",
      "  [-10.52 -13.84   -inf]]\n",
      "\n",
      " [[  0.     0.    11.29]\n",
      "  [  0.     0.     0.  ]\n",
      "  [  0.     0.01  -7.02]\n",
      "  [  0.     0.     0.  ]\n",
      "  [  0.     2.43   -inf]]\n",
      "\n",
      " [[  0.     0.     0.  ]\n",
      "  [  0.     0.     0.  ]\n",
      "  [  0.     0.     0.  ]\n",
      "  [  0.     0.     0.  ]\n",
      "  [  0.     0.     -inf]]]\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.01             # Exponential decay rate for exploration prob\n",
    "\n",
    "learning_rate = 0.7\n",
    "discount_rate = 0.618\n",
    "\n",
    "total_episodes = 300\n",
    "steps = 300\n",
    "\n",
    "world_size = (5, 5)\n",
    "\n",
    "# Init Q table\n",
    "q_table = np.zeros((5, 5, 3))\n",
    "# If you see border, then you should not go forward\n",
    "for i in range(5):\n",
    "    q_table[i][States.BORDER.value][Actions.FORWARD.value] = -math.inf\n",
    "#print(q_table)\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    env = Covid19Environment(q_table, epsilon, generate_random_others(2), world_size)\n",
    "    for s in range(steps):\n",
    "        done, new_reward = env.step()\n",
    "        total_reward += new_reward\n",
    "        # If done : finish episode\n",
    "        if done == True:\n",
    "            break\n",
    "    #print(episode, \" Finished: \", s, \", reward: \", total_reward, \"Random/total actions: \", (env.random_actions*100)/env.total_actions)\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "print(\"\\n\\nQ Table:\\n\", q_table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play it\n",
    "Here you can run a graphical representation of what is happening (**see pop-up window**). It uses learned Q table to make decisions. You can move yourself by pressing forward, left and right or **press down to make a decision based on the Q table**.\n",
    "\n",
    "- <span style=\"color:green\">Kiwi is a green rectangle</span>\n",
    "- <span style=\"color:brown\">House is a brown rectangle</span>\n",
    "- <span style=\"color:orange\">Others are orange rectangles</span>\n",
    "- Body of surviver is white and what it sees is a different colour blue\n",
    "\n",
    "<img src=\"covid_graphics1.png\" alt=\"Graphical representation of the game\" width=\"200\"/>\n",
    "\n",
    "<img src=\"covid_graphics2.png\" alt=\"Graphical representation near the house\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PlottingGrid\n",
      "FinishedPlotting\n",
      "House added\n",
      "Kiwi added\n",
      "Survivor added\n",
      "Others are added\n",
      "States.NOTHING States.NOTHING\n",
      "States.NOTHING States.NOTHING\n",
      "States.NOTHING States.NOTHING\n",
      "States.NOTHING States.BORDER\n",
      "States.NOTHING States.NOTHING\n",
      "States.NOTHING States.NOTHING\n",
      "States.NOTHING States.NOTHING\n",
      "States.NOTHING States.KIWI\n",
      "States.KIWI States.BORDER\n",
      "States.KIWI States.NOTHING\n",
      "States.NOTHING States.NOTHING\n",
      "States.NOTHING States.NOTHING\n",
      "States.NOTHING States.NOTHING\n",
      "States.NOTHING States.NOTHING\n",
      "States.NOTHING States.NOTHING\n",
      "States.NOTHING States.NOTHING\n",
      "States.NOTHING States.NOTHING\n",
      "States.NOTHING States.NOTHING\n",
      "States.NOTHING States.BORDER\n",
      "States.NOTHING States.NOTHING\n",
      "States.NOTHING States.NOTHING\n",
      "States.NOTHING States.NOTHING\n",
      "States.NOTHING States.NOTHING\n",
      "States.NOTHING States.NOTHING\n",
      "States.NOTHING States.NOTHING\n",
      "States.NOTHING States.NOTHING\n",
      "States.NOTHING States.NOTHING\n",
      "States.NOTHING States.HOUSE\n",
      "States.HOUSE States.BORDER\n",
      "Wooow! House reached after Kiwi! 224\n",
      "Total reward:  224\n"
     ]
    }
   ],
   "source": [
    "# Graphical representation of the game\n",
    "world_size = (10, 10)\n",
    "env = GraphicCovid19Environment(generate_random_others(2), True, q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A surprising observation for me was that it is better to train the model on a 5x5 environment, since it is a bigger chance it had to learn to find Kiwi before coming home. And then applying that knowledge on a bigger world. It makes sense that the change of the size of the world does not matter so much since the agent only knows 2 things (its position and what is in front)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep QL\n",
    "\n",
    "Using Q-table shows its weaknesses in covid19 environment, because the survivor can easily end up going in circles when the board gets bigger. It does not really remember well where the survivor has been. Thus it would have been more benefitial to use Deep QL.\n",
    "\n",
    "In this case one could have easily used the environment table as a representation of a camera seeing above the grid. One would need to update the survivor's position each step. To make it more interesting we could make \"others\" move around and thus sending 2 tables at each step into a neural network that outputs 3 values (probabilities of taking the 3 actions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0],\n",
       "       [0, 2, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 2, 0, 9, 0],\n",
       "       [0, 0, 0, 0, 3]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.environment_table[3, 3] = 9 # place holder for survivor position\n",
    "env.environment_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in updating Q table and weights would be as follows:\n",
    "\n",
    " 1. Choosing an action from the current state depending on the random epsilon\n",
    " 2. Taking that action, finding reward and next state after that\n",
    " 3. Finding the maximum possible reward from that new state\n",
    "\n",
    "For Q-table:\n",
    "\n",
    "4.Update the q-table with the new q value which is: \n",
    " $$current_Q_value + learning_rate * (reward + discount_rate * highest_Q_value - current_Q_value)$$\n",
    "For Deep QL:\n",
    "\n",
    "4.Update the weights: \n",
    " $$learning_rate * (reward + discount_rate * highest_Q_value) * âˆ‡_weights_Q_value$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "- [Free code camp](https://www.freecodecamp.org/news/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe/)\n",
    "- [Taxi](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/Taxi-v2/Q%20Learning%20with%20OpenAI%20Taxi-v2%20video%20version.ipynb)\n",
    "- [Q Tables](https://itnext.io/reinforcement-learning-with-q-tables-5f11168862c8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
